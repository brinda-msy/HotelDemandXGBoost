# HotelDemandXGBoost
Predictive Modelling using Ensemble techniques
Ensemble models combine predictions from different models in order to increase outcome accuracy (Brownlee, 2016). Bagging, Boosting and Voting are three most popular methos to combine predictions from different models that are already built. 
Random forest classifier is a bagging/averaging method where average from predictions based on several estimators are considered because the variance is reduced. It is a modified learner with a number of decision trees within it (Farooq et al, 2021). The RFC model was tried by specifying various values for n_estimators (the number of trees to build while taking average of predictions) and the default value of 100 was found to have best accuracy. The quality of the split was measured using Gini impurity (criterion = gini) and depth of the trees was until leaves are pure (max_depth=None). Minimum two samples were needed for a node to be split (min_samples_split = 2). 
XGBoost is a comparatively fast boosting ensemble model and has proven to have better benchmarking (Pathak, 2019). The outcome of weak learners is weighted based on accuracy and the errors of previous models are reduced. In order to use XGBoost model, the categorical data points in the data set were encoded before training the model. The same encoded data set was used to build other models as well. The default base learners ‘gbtrees’ trees was used. In order to avoid over fitting, maximum depth of tree was set at 6 (max_depth). Evaluation metric of ‘error’ was used with 0.5 as classification error rate. 
![image](https://user-images.githubusercontent.com/57647172/117545516-5ec2c280-b026-11eb-820e-cbcb2193ca6d.png)
